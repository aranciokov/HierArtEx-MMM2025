{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbf93b28-66be-46f5-a176-4d55db6f5f81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 244, 2066, 2390,  491, 1661, 1910,  432, 1716,  561, 1514]),\n",
       " tensor([1512, 1029,  136, 1034, 2774,  853,  844, 1302, 1699, 2537]),\n",
       " tensor([  87,  376, 2034,  228, 2892,  125, 2048, 1906, 2734, 2307]))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "indices = pd.read_pickle(\"indices_museum_dataset.pkl\")\n",
    "indices['train'][:10], indices['val'][:10], indices['test'][:10],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4df9da4-a5d8-4169-940b-cc2b9f332961",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import os\n",
    "\n",
    "class DescriptionSceneMuseum(Dataset):\n",
    "    def __init__(self, data_description_path, data_raw_description_path, data_scene_path, data_art_path, indices, split, customized_margin=False):\n",
    "        self.description_path = data_description_path\n",
    "        self.raw_description_path = data_raw_description_path\n",
    "        self.data_pov_path = data_scene_path\n",
    "        self.indices = indices[split]\n",
    "        self.split = split\n",
    "\n",
    "        available_data = [im.strip(\".pt\") for im in os.listdir(data_scene_path)]\n",
    "        available_data = sorted(available_data)\n",
    "        available_data = [available_data[ix] for ix in self.indices.tolist()]\n",
    "\n",
    "        self.descs = [torch.load(os.path.join(data_description_path, f\"{sm}.pt\")) for sm in available_data]\n",
    "        self.raw_descs = [\" \".join(pd.read_pickle(os.path.join(data_raw_description_path, f\"{sm}.pkl\"))) for sm in available_data]\n",
    "        self.pov_images = [torch.load(os.path.join(data_scene_path, f\"{sm}.pt\")) for sm in available_data]\n",
    "        self.art_vectors = [torch.load(os.path.join(data_art_path, f\"{sm}.pt\")) for sm in available_data]\n",
    "        self.names = available_data\n",
    "        print(f\"'{split.upper()}': {len(self.names)} names, \"\n",
    "              f\"{len(self.descs)} sentences ({sum([len(x) for x in self.descs]) / len(self.descs)} avg), \"\n",
    "              f\"{len(self.pov_images)} images ({sum([len(x) for x in self.pov_images]) / len(self.pov_images)} avg).\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.names)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        desc_tensor = self.descs[index]\n",
    "        raw_desc = self.raw_descs[index]\n",
    "        scene_img_tensor = self.pov_images[index]\n",
    "        scene_art_tensor = self.art_vectors[index]\n",
    "        name = self.names[index]\n",
    "\n",
    "        return desc_tensor, scene_img_tensor, scene_art_tensor, raw_desc, name, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b7cfac6-5694-4a48-858b-0f9601c0329f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "visual_backbone = \"rn50\"\n",
    "device = \"cuda:0\"\n",
    "visual_bb_ftsize_k = {'rn18': 512, 'rn34': 512, 'rn50': 2048, 'rn101': 2048, 'vitb16': 768, 'vitb32': 768, 'openclip': 512}\n",
    "visual_bb_ftsize = visual_bb_ftsize_k[visual_backbone]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3f3082c-fe28-4217-a87f-55ef2cdcd0fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'TEST': 450 names, 450 sentences (170.16444444444446 avg), 450 images (77.38666666666667 avg).\n"
     ]
    }
   ],
   "source": [
    "test_dataset = DescriptionSceneMuseum(\"./tmp_museums/open_clip_features_museums3k/descriptions/sentences\", \n",
    "                                   \"./tmp_museums/open_clip_features_museums3k/descriptions/tokens_strings\", \n",
    "                                   \"./tmp_museums/open_clip_features_museums3k/images\",\n",
    "                                   f\"./preextracted_vectors_wikiart_{visual_backbone}\",\n",
    "                            indices, \"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dec2f64f-927c-4169-afb1-df7ae2796890",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class MyHierBaseline_v2(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, feature_size, art_features_size=2048):\n",
    "        super().__init__()\n",
    "        self.trf_photo = nn.Linear(in_channels+art_features_size, out_channels)\n",
    "        self.trf_room = nn.Linear(out_channels, out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.trf_museum = nn.Linear(out_channels, feature_size)\n",
    "\n",
    "    def forward(self, x, x_art, list_length=None, clip_mask=None, imgs_per_room=None):\n",
    "        x = x.to(torch.float32)\n",
    "        \n",
    "        x1 = self.trf_photo(torch.cat((\n",
    "            x.transpose(1, 2), \n",
    "            x_art.transpose(1, 2)\n",
    "        ), -1))\n",
    "        \n",
    "        if clip_mask is not None:\n",
    "            x1 = x1 * clip_mask\n",
    "        # remove the effect of the padding\n",
    "        if list_length is not None:\n",
    "            for item_idx in range(x.shape[0]):\n",
    "                x1[item_idx, list_length[item_idx]:, :] = 0\n",
    "        x1_img = self.relu(x1)\n",
    "        \n",
    "        bsz, max_n_imgs, ft_size = x1_img.shape\n",
    "        if isinstance(imgs_per_room, int):\n",
    "            n_rooms = max_n_imgs // imgs_per_room\n",
    "            x1_room = x1_img.view(bsz, n_rooms, imgs_per_room, ft_size)\n",
    "            \n",
    "            # we aggregate the \"image-level\" information, and learn room-level information\n",
    "            x1_room = x1_room.mean(2)\n",
    "            x1_room = self.trf_room(x1_room)\n",
    "            x1_room = self.relu(x1_room)\n",
    "            \n",
    "            # then, we aggregate the room-level info, and learn a museum-level representation\n",
    "            assert list_length is not None\n",
    "            x1_museum = x1_room.clone()\n",
    "            for item_idx in range(x.shape[0]):\n",
    "                x1_museum[item_idx, list_length[item_idx]:, :] = 0\n",
    "            list_length_t = torch.tensor(list_length, device=x1_room.device) if isinstance(list_length, list) else list_length.to(x1_room.device)\n",
    "            x1_museum = x1_museum.sum(1) / (list_length_t / imgs_per_room).view(-1, 1)\n",
    "            x1_museum = self.trf_museum(x1_museum)\n",
    "        \n",
    "        else:\n",
    "            assert False, imgs_per_room\n",
    "            x1 = x1.sum(1) / (x1.sum(-1) > 0).sum(1).unsqueeze(-1)\n",
    "            x1 = x1.view(x1.size(0), -1)\n",
    "            x1 = self.trf_museum(x1)\n",
    "        return x1_img, x1_room, x1_museum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b5a1ec6-dc25-48a9-a944-9b014267580b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GRUNet(nn.Module):\n",
    "    def __init__(self, hidden_size, num_features, is_bidirectional=False):\n",
    "        super(GRUNet, self).__init__()\n",
    "        self.gru = nn.GRU(input_size=num_features, hidden_size=hidden_size, batch_first=True,\n",
    "                          bidirectional=is_bidirectional)\n",
    "        self.is_bidirectional = is_bidirectional\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(torch.float32)\n",
    "        _, h_n = self.gru(x)\n",
    "        if self.is_bidirectional:\n",
    "            return h_n.mean(0)\n",
    "        return h_n.squeeze(0)\n",
    "    \n",
    "class MyBaseline(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.trf_photo = nn.Linear(in_channels, out_channels)\n",
    "        self.trf_mean = nn.Linear(out_channels, out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, list_length=None, clip_mask=None, imgs_per_room=None):\n",
    "        x = x.to(torch.float32)\n",
    "        \n",
    "        x1 = self.trf_photo(x.transpose(1, 2))\n",
    "        \n",
    "        if clip_mask is not None:\n",
    "            x1 = x1 * clip_mask\n",
    "        # remove the effect of the padding\n",
    "        if list_length is not None:\n",
    "            for item_idx in range(x.shape[0]):\n",
    "                x1[item_idx, list_length[item_idx]:, :] = 0\n",
    "        x1_img = self.relu(x1)\n",
    "        \n",
    "        bsz, max_n_imgs, ft_size = x1_img.shape\n",
    "        list_length_t = torch.tensor(list_length, device=x1_img.device) if isinstance(list_length, list) else list_length.to(x1_img.device)\n",
    "        x1_mean = x1_img.sum(1) / list_length_t.unsqueeze(1)\n",
    "        x1_museum = self.trf_mean(x1_mean)\n",
    "        return x1_museum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f82dbb27-9207-4473-91d6-0868622f7f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "def cosine_sim(im, s):\n",
    "    '''cosine similarity between all the image and sentence pairs\n",
    "    '''\n",
    "    inner_prod = im.mm(s.t())\n",
    "    im_norm = torch.sqrt((im ** 2).sum(1).view(-1, 1) + 1e-18)\n",
    "    s_norm = torch.sqrt((s ** 2).sum(1).view(1, -1) + 1e-18)\n",
    "    sim = inner_prod / (im_norm * s_norm)\n",
    "    return sim\n",
    "\n",
    "\n",
    "def create_rank(result, entire_descriptor, desired_output_index):\n",
    "    similarity = torch.nn.functional.cosine_similarity(entire_descriptor, result, dim=1)\n",
    "    similarity = similarity.squeeze()\n",
    "    sorted_indices = torch.argsort(similarity, descending=True)\n",
    "    position = torch.where(sorted_indices == desired_output_index)\n",
    "    return position[0].item(), sorted_indices\n",
    "\n",
    "\n",
    "def evaluate(output_description, output_scene, section, out_values=False, excel_format=False):\n",
    "    avg_rank_scene = 0\n",
    "    ranks_scene = []\n",
    "    avg_rank_description = 0\n",
    "    ranks_description = []\n",
    "\n",
    "    ndcg_10_list = []\n",
    "    ndcg_entire_list = []\n",
    "\n",
    "    for j, i in enumerate(output_scene):\n",
    "        rank, sorted_list = create_rank(i, output_description, j)\n",
    "        avg_rank_scene += rank\n",
    "        ranks_scene.append(rank)\n",
    "\n",
    "    for j, i in enumerate(output_description):\n",
    "        rank, sorted_list = create_rank(i, output_scene, j)\n",
    "        avg_rank_description += rank\n",
    "        ranks_description.append(rank)\n",
    "\n",
    "    ranks_scene = np.array(ranks_scene)\n",
    "    ranks_description = np.array(ranks_description)\n",
    "\n",
    "    n_q = len(output_scene)\n",
    "    sd_r1 = 100 * len(np.where(ranks_scene < 1)[0]) / n_q\n",
    "    sd_r5 = 100 * len(np.where(ranks_scene < 5)[0]) / n_q\n",
    "    sd_r10 = 100 * len(np.where(ranks_scene < 10)[0]) / n_q\n",
    "    sd_medr = np.median(ranks_scene) + 1\n",
    "    sd_meanr = ranks_scene.mean() + 1\n",
    "\n",
    "    n_q = len(output_description)\n",
    "    ds_r1 = 100 * len(np.where(ranks_description < 1)[0]) / n_q\n",
    "    ds_r5 = 100 * len(np.where(ranks_description < 5)[0]) / n_q\n",
    "    ds_r10 = 100 * len(np.where(ranks_description < 10)[0]) / n_q\n",
    "    ds_medr = np.median(ranks_description) + 1\n",
    "    ds_meanr = ranks_description.mean() + 1\n",
    "\n",
    "    ds_out, sc_out = \"\", \"\"\n",
    "    for mn, mv in [[\"R@1\", ds_r1],\n",
    "                   [\"R@5\", ds_r5],\n",
    "                   [\"R@10\", ds_r10],\n",
    "                   [\"median rank\", ds_medr],\n",
    "                   [\"mean rank\", ds_meanr],\n",
    "                   ]:\n",
    "        ds_out += f\"{mn}: {mv:.4f}   \"\n",
    "\n",
    "    for mn, mv in [(\"R@1\", sd_r1),\n",
    "                   (\"R@5\", sd_r5),\n",
    "                   (\"R@10\", sd_r10),\n",
    "                   (\"median rank\", sd_medr),\n",
    "                   (\"mean rank\", sd_meanr),\n",
    "                   ]:\n",
    "        sc_out += f\"{mn}: {mv:.4f}   \"\n",
    "\n",
    "    if out_values:\n",
    "        print(section + \" data: \")\n",
    "        print(\"Scenes ranking: \" + ds_out)\n",
    "        print(\"Descriptions ranking: \" + sc_out)\n",
    "    if section == \"test\" and len(ndcg_10_list) > 0:\n",
    "        avg_ndcg_10_entire = 100 * sum(ndcg_10_list) / len(ndcg_10_list)\n",
    "        avg_ndcg_entire = 100 * sum(ndcg_entire_list) / len(ndcg_entire_list)\n",
    "    else:\n",
    "        avg_ndcg_10_entire = -1\n",
    "        avg_ndcg_entire = -1\n",
    "    \n",
    "    if excel_format:\n",
    "        print(\"-\"*5)\n",
    "        print(\"{ds_r1};{ds_r5};{ds_r10};{sd_r1};{sd_r5};{sd_r10};{ds_medr};{sd_medr}\")\n",
    "        print(f\"{ds_r1};{ds_r5};{ds_r10};{sd_r1};{sd_r5};{sd_r10};{ds_medr};{sd_medr}\")\n",
    "        print(\"-\"*5)\n",
    "        formatted_string = f\"{ds_r1};{ds_r5};{ds_r10};{sd_r1};{sd_r5};{sd_r10};{ds_medr};{sd_medr}\"\n",
    "        return ds_r1, ds_r5, ds_r10, sd_r1, sd_r5, sd_r10, avg_ndcg_10_entire, avg_ndcg_entire, ds_medr, sd_medr, formatted_string        \n",
    "    \n",
    "    return ds_r1, ds_r5, ds_r10, sd_r1, sd_r5, sd_r10, avg_ndcg_10_entire, avg_ndcg_entire, ds_medr, sd_medr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0963f4-2c72-49b2-8fbd-90373f665847",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "646e4aa8-f9dc-4439-b50f-58aca489db27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyHierBaseline_v2(\n",
       "  (trf_photo): Linear(in_features=2560, out_features=256, bias=True)\n",
       "  (trf_room): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (trf_museum): Linear(in_features=256, out_features=256, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_vis = torch.load(\"models/TQFQD/hierarchical_v2_art_vectors_rn50_1.pt\")\n",
    "model_pov = MyHierBaseline_v2(in_channels=512, out_channels=256, feature_size=256, art_features_size=visual_bb_ftsize)\n",
    "\n",
    "model_pov.load_state_dict(weights_vis['best_model_0'])\n",
    "model_pov.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70814340-0e88-4bf0-b1cd-876b55da96f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GRUNet(\n",
       "  (gru): GRU(512, 256, batch_first=True, bidirectional=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_desc_pov = GRUNet(hidden_size=256, num_features=512, is_bidirectional=True)\n",
    "model_desc_pov.load_state_dict(weights_vis['best_model_1'])\n",
    "model_desc_pov.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67ca5f87-80da-4e7b-8e08-9c75ed035176",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "def collate_fn(data):  # data -> desc_tensor, scene_img_tensor, name, index\n",
    "    raw_descs = False\n",
    "    adj = 0\n",
    "    if len(data[0]) == 6:  # train -> raw descriptions\n",
    "        raw_descs = True\n",
    "        adj = 1\n",
    "\n",
    "    tmp_description_povs = [x[0] for x in data]\n",
    "    tmp = pad_sequence(tmp_description_povs, batch_first=True)\n",
    "    descs_pov = pack_padded_sequence(tmp,\n",
    "                                     torch.tensor([len(x) for x in tmp_description_povs]),\n",
    "                                     batch_first=True,\n",
    "                                     enforce_sorted=False)\n",
    "\n",
    "    tmp_pov = [x[1] for x in data]\n",
    "    len_pov = torch.tensor([len(x) for x in tmp_pov])\n",
    "    padded_pov = pad_sequence(tmp_pov, batch_first=True)\n",
    "    padded_pov = torch.transpose(padded_pov, 1, 2)\n",
    "\n",
    "    tmp_art = [x[2] for x in data]\n",
    "    len_art = torch.tensor([len(x) for x in tmp_art])\n",
    "    padded_art = pad_sequence(tmp_art, batch_first=True)\n",
    "    padded_art = torch.transpose(padded_art, 1, 2)\n",
    "\n",
    "    if raw_descs:\n",
    "        raw_descs = [x[3] for x in data]\n",
    "    names = [x[3+adj] for x in data]\n",
    "    indexes = [x[4+adj] for x in data]\n",
    "    \n",
    "    if raw_descs:\n",
    "        return descs_pov, padded_pov, padded_art, raw_descs, names, indexes, len_pov\n",
    "    else:\n",
    "        return descs_pov, padded_pov, padded_art, names, indexes, len_pov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2cb50fde-3f11-4df5-888f-038f7323675c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size = 32\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=False, num_workers=4)\n",
    "test_names = list()\n",
    "model_pov.eval()\n",
    "model_desc_pov.eval()\n",
    "output_description_test = torch.empty(len(indices['test']), 256)\n",
    "output_pov_test = torch.empty(len(indices['test']), 256)\n",
    "with torch.no_grad():\n",
    "    for j, (data_desc_pov, data_pov, data_art, raw_descs, names, indexes, len_pov) in enumerate(test_loader):\n",
    "\n",
    "        data_desc_pov = data_desc_pov.to(device)\n",
    "        data_pov = data_pov.to(device)\n",
    "        data_art = data_art.to(device)\n",
    "\n",
    "        test_names.extend(names)\n",
    "\n",
    "        bsz, fts, no_room_times_no_imgs = data_pov.shape\n",
    "\n",
    "        output_desc_pov = model_desc_pov(data_desc_pov)\n",
    "        output_pov_img_level, output_pov_room_level, output_pov = model_pov(data_pov, data_art, len_pov, imgs_per_room=12)\n",
    "\n",
    "        initial_index = j * batch_size\n",
    "        final_index = (j + 1) * batch_size\n",
    "        if final_index > len(indices['test']):\n",
    "            final_index = len(indices['test'])\n",
    "        output_description_test[initial_index:final_index, :] = output_desc_pov\n",
    "        output_pov_test[initial_index:final_index, :] = output_pov\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8d3e9e3-adb6-40ba-ab91-06b31508620e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([450, 256])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_pov_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6ff2f47-e5ad-4fa8-a0ec-c2894a3ffb9b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48.666666666666664,\n",
       " 82.88888888888889,\n",
       " 91.33333333333333,\n",
       " 45.55555555555556,\n",
       " 81.55555555555556,\n",
       " 90.22222222222223,\n",
       " -1,\n",
       " -1,\n",
       " 2.0,\n",
       " 2.0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(output_description_test, output_pov_test, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2283ed81-9695-48b7-b356-7d7c750019fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import open_clip\n",
    "clip, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32-quickgelu', pretrained='laion400m_e32')\n",
    "tokenizer = open_clip.get_tokenizer('ViT-B-32-quickgelu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d196185-52d6-439d-b7fe-279f00d64967",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cosine_sim(im, s):\n",
    "    '''cosine similarity between all the image and sentence pairs\n",
    "    '''\n",
    "    inner_prod = im.mm(s.t())\n",
    "    im_norm = torch.sqrt((im ** 2).sum(1).view(-1, 1) + 1e-18)\n",
    "    s_norm = torch.sqrt((s ** 2).sum(1).view(1, -1) + 1e-18)\n",
    "    sim = inner_prod / (im_norm * s_norm)\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4cf687a-3844-4c06-866a-7ecf2c3f815e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/afalcon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/afalcon/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import string\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a83f972d-b58e-4b2b-b78a-b1ca0558c8b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weights_vis = torch.load(\"models/L6U2M/mean_pool_baseline_2.pt\")\n",
    "model_pov_base = MyBaseline(in_channels=512, out_channels=256)\n",
    "model_pov_base.load_state_dict(weights_vis['best_model_0'])\n",
    "model_pov_base.to(device)\n",
    "\n",
    "model_desc_pov_base = GRUNet(hidden_size=256, num_features=512, is_bidirectional=True)\n",
    "model_desc_pov_base.load_state_dict(weights_vis['best_model_1'])\n",
    "model_desc_pov_base.to(device)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "batch_size = 32\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=False, num_workers=4)\n",
    "test_names = list()\n",
    "model_pov_base.eval()\n",
    "model_desc_pov_base.eval()\n",
    "output_description_test_base = torch.empty(len(indices['test']), 256)\n",
    "output_pov_test_base = torch.empty(len(indices['test']), 256)\n",
    "with torch.no_grad():\n",
    "    for j, (data_desc_pov, data_pov, data_art, raw_descs, names, indexes, len_pov) in enumerate(test_loader):\n",
    "\n",
    "        data_desc_pov = data_desc_pov.to(device)\n",
    "        data_pov = data_pov.to(device)\n",
    "        data_art = data_art.to(device)\n",
    "\n",
    "        test_names.extend(names)\n",
    "\n",
    "        bsz, fts, no_room_times_no_imgs = data_pov.shape\n",
    "\n",
    "        output_desc_pov = model_desc_pov_base(data_desc_pov)\n",
    "        output_pov = model_pov_base(data_pov, len_pov)\n",
    "\n",
    "        initial_index = j * batch_size\n",
    "        final_index = (j + 1) * batch_size\n",
    "        if final_index > len(indices['test']):\n",
    "            final_index = len(indices['test'])\n",
    "        output_description_test_base[initial_index:final_index, :] = output_desc_pov\n",
    "        output_pov_test_base[initial_index:final_index, :] = output_pov\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0580ef34-2cba-4c0c-8119-2d5eb20ca314",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48.666666666666664,\n",
       " 82.88888888888889,\n",
       " 91.33333333333333,\n",
       " 45.55555555555556,\n",
       " 81.55555555555556,\n",
       " 90.22222222222223,\n",
       " -1,\n",
       " -1,\n",
       " 2.0,\n",
       " 2.0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(output_description_test, output_pov_test, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77ea79ee-80b8-4bfa-81d9-9cc39a2c97d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21.77777777777778,\n",
       " 50.888888888888886,\n",
       " 69.55555555555556,\n",
       " 21.11111111111111,\n",
       " 49.111111111111114,\n",
       " 64.88888888888889,\n",
       " -1,\n",
       " -1,\n",
       " 5.0,\n",
       " 6.0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(output_description_test_base, output_pov_test_base, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0136367-1317-4dce-a4d8-59351f6e80b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff7a7ad6-0fea-4083-a414-df1ccc292321",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_most_common = 5\n",
    "n_top_k = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b9a8703-e148-4436-8492-6e6675f6af1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['landscape, religious, life, still, italian', 'life, portrait, french, still, altarpiece', 'portrait, italian, landscape, religious, century', 'christ, portrait, mary, italian, flemish', 'portrait, life, landscape, italian, studio']\n"
     ]
    }
   ],
   "source": [
    "gt_concepts = []\n",
    "bad_words =  [\n",
    "    '1533', 'painting', 'paintings', 'one', 'two', 'titled', 'described', 'follows', 'work', 'room', 'picture', 'csontv', 'last', 'late', 'later', 'ois', 'period',\n",
    "    'right', 'left', 'panels', 'painted', 'scene', 'first', '\\'1\\'', '1', 'figures', 'scenes', 'shows', 'style', 'school', '``', 'elements', 'form', 'found', 'seven', 'may',\n",
    "    \"''\", 'wall', '2', 'st', 'van', 'made', 'head', 'di', 'genre', 'four', 'three', 'panel', 'also', 'artist', 'new', 'de', 'commissioned', 'frame', 'half', 'image',\n",
    "    'pictures', 'self', 'considered', 'view', 'grand', 'main', 'painter', 'rer', 'end', 'art', 'signed', 'subject', 'figure', 'figures', 'first', 'second', 'following',\n",
    "    'side', 'whose', 'seen', 'known', '000', '3', 'acts', 'age', 'almost', 'another', 'although', 'appears', 'around', 'artists', 'background', 'behind', 'brothers', 'c', \n",
    "    'centre', 'ceiling', 'central', 'could', 'corner', 'da', 'dated', 'del', 'dell', 'della', 'der', 'du', 'evident', 'f', 'example', 'famous', 'five', 'foreground', 'many',\n",
    "    'fully', 'g', 'good', 'great', 'high', 'however', 'ii', 'important', 'influence', 'le', 'like', 'little', 'lot', 'lotto', 'often', 'painters', 'placed', 'probably', 'use',\n",
    "    'produced', 'represented', 'represents', 'representing', 'shown', 'six', 'time', 'toward', 'towards', 'twenty', 'way', 'works', 'depicted', 'depicts', 'different', 'see',\n",
    "    'us', 'career', 'collection', 'composition', 'construction', 'early', 'even', 'exchange', 'master', 'middle', 'museum', 'rard', 'rati', 'several', 'sts', 'ry', 'version',\n",
    "    'viewer', 'year', 'years', '25', '35', '6', 'among', 'along', 'appear', 'attributed', 'back', 'based', 'became', 'began', 'belonged', 'beside', 'bottom', 'called', 'calling', 'came',\n",
    "    'characters', 'cm', 'compartments', 'created', 'dei', 'depicting', 'detail', 'displayed', 'either', 'el', 'earlier', 'es', 'especially', 'existence', 'features', 'exterior', 'interior',\n",
    "    'holding', 'identified', 'intended', 'length', 'lent', 'lived', 'long', 'ly', 'madame', 'much', 'near', 'perhaps', 'place', 'point', 'popular', 'portrayed', 'r', 'rather', 'rd',\n",
    "    'raire', 'register', 'return', 'reverse', 'rooms', 'seem', 'show', 'sides', 'similar','something', 'spectator', 'state', 'study', 'subjects', 'surrounded', 'sz',\n",
    "    'taken', 'though', 'together', 'upper', 'v', 'von', 'without', 'would', 'x'\n",
    "]\n",
    "\n",
    "def filter_proc(_words, most_common=15):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word for word in _words if word not in stop_words and word not in string.punctuation and word not in bad_words]\n",
    "\n",
    "    word_counts = Counter(filtered_words)\n",
    "\n",
    "    most_common_words = word_counts.most_common(most_common)\n",
    "    most_common_words = [n for (n, c) in most_common_words]\n",
    "    return most_common_words\n",
    "\n",
    "for ix in range(450):\n",
    "    raw_desc = test_dataset[ix][-3]\n",
    "    words = word_tokenize(raw_desc.lower())\n",
    "\n",
    "    most_common_words = filter_proc(words, n_most_common)\n",
    "    for _ in range(n_top_k):\n",
    "        gt_concepts.append(\", \".join(most_common_words))\n",
    "\n",
    "print(gt_concepts[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78e67f37-7165-4d7f-9ab5-89eb67474fe2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98\n"
     ]
    }
   ],
   "source": [
    "baseline_concepts = []\n",
    "top1r = 0\n",
    "for ix in range(450):\n",
    "    query = test_dataset[ix][0]\n",
    "    query_fts = model_desc_pov_base(query.to(device))\n",
    "\n",
    "    ext_res = torch.topk(cosine_sim(output_pov_test_base.to(device), query_fts.float().unsqueeze(0)), k=n_top_k, dim=0)\n",
    "    if ext_res[1][0].item() == ix:\n",
    "        top1r += 1\n",
    "    for other_ix in ext_res[1]:\n",
    "        rd = test_dataset[other_ix.item()][-3]\n",
    "        words = word_tokenize(rd.lower())\n",
    "        most_common_words = filter_proc(words, n_most_common)\n",
    "        baseline_concepts.append(\", \".join(most_common_words))\n",
    "print(top1r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6bb92414-b739-4bd5-b11e-916bfc2d558b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219\n"
     ]
    }
   ],
   "source": [
    "hierartex_concepts = []\n",
    "_top_1_indices = []\n",
    "_top_1_museum_names = []\n",
    "top1r = 0\n",
    "for ix in range(450):\n",
    "    query = test_dataset[ix][0]\n",
    "    query_fts = model_desc_pov(query.to(device))\n",
    "\n",
    "    ext_res = torch.topk(cosine_sim(output_pov_test.to(device), query_fts.float().unsqueeze(0)), k=n_top_k, dim=0)\n",
    "    _top_1_indices.append(ext_res[1][0].item())\n",
    "    _top_1_museum_names.append(test_dataset[ext_res[1][0].item()][-2])\n",
    "    if ext_res[1][0].item() == ix:\n",
    "        top1r += 1\n",
    "        \n",
    "    for other_ix in ext_res[1]:\n",
    "        rd = test_dataset[other_ix.item()][-3]\n",
    "        words = word_tokenize(rd.lower())\n",
    "        most_common_words = filter_proc(words, n_most_common)\n",
    "        hierartex_concepts.append(\", \".join(most_common_words))\n",
    "\n",
    "print(top1r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "174d668c-b4b3-4f7c-9da9-3a0259fe86ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wins 228, draws 116, losses 106\n",
      "228/116/106\n"
     ]
    }
   ],
   "source": [
    "def count_matches(gt, pred):\n",
    "    cnt = 0\n",
    "    for concept in gt.split(\",\"):\n",
    "        for p in pred.split(\",\"):\n",
    "            if concept.strip() in p.strip():\n",
    "                cnt += 1\n",
    "    return cnt\n",
    "\n",
    "wins, draws, losses = 0, 0, 0\n",
    "if n_top_k == 1:\n",
    "    for ix in range(450):\n",
    "        h_cn = count_matches(gt_concepts[ix], hierartex_concepts[ix])\n",
    "        b_cn = count_matches(gt_concepts[ix], baseline_concepts[ix])\n",
    "        \n",
    "        if h_cn > b_cn:\n",
    "            wins += 1\n",
    "        elif h_cn == b_cn:\n",
    "            draws += 1\n",
    "        elif b_cn > h_cn:\n",
    "            losses += 1\n",
    "\n",
    "else:        \n",
    "    for ix in range(450):\n",
    "        cum_h, cum_b = 0, 0\n",
    "        for it in range(n_top_k):\n",
    "            h_cn = count_matches(gt_concepts[ix*n_top_k+it], hierartex_concepts[ix*n_top_k+it])\n",
    "            b_cn = count_matches(gt_concepts[ix*n_top_k+it], baseline_concepts[ix*n_top_k+it])\n",
    "            cum_h += h_cn\n",
    "            cum_b += b_cn\n",
    "            \n",
    "        if cum_h > cum_b:\n",
    "            wins += 1\n",
    "        elif cum_h == cum_b:\n",
    "            draws += 1\n",
    "        elif cum_b > cum_h:\n",
    "            losses += 1\n",
    "\n",
    "print(f\"wins {wins}, draws {draws}, losses {losses}\")\n",
    "print(f\"{wins}/{draws}/{losses}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2327ef62-9bff-40ca-a5ea-bee09168a9b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d1b2e441-2cae-4f6d-af90-2d540d1a1fa7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(190,\n",
       " 'Museum1798-7.unity',\n",
       " 'portrait, landscape, italian, christ, life',\n",
       " 'french, italian, portrait, religious, life')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosen_idx = 255  # 69, 42, 255\n",
    "_top_1_indices[chosen_idx], _top_1_museum_names[chosen_idx], gt_concepts[chosen_idx], hierartex_concepts[chosen_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcfb255-f135-44a0-bee8-bf16b6939841",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743496ae-d082-40b2-9fc5-a7221c3920f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8eaf2247-fab7-441c-8ac4-e191b43e4bd3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'adam',\n",
       " 'aeneas',\n",
       " 'agnes',\n",
       " 'altar',\n",
       " 'altarpiece',\n",
       " 'andrew',\n",
       " 'angels',\n",
       " 'angeluccio',\n",
       " 'anguissola',\n",
       " 'anthony',\n",
       " 'apollo',\n",
       " 'arms',\n",
       " 'baptist',\n",
       " 'barak',\n",
       " 'bardi',\n",
       " 'begat',\n",
       " 'bianca',\n",
       " 'blind',\n",
       " 'blue',\n",
       " 'bridge',\n",
       " 'campin',\n",
       " 'cano',\n",
       " 'caravaggio',\n",
       " 'cardinal',\n",
       " 'carlevaris',\n",
       " 'carlo',\n",
       " 'catherine',\n",
       " 'cecilia',\n",
       " 'cell',\n",
       " 'century',\n",
       " 'chapel',\n",
       " 'child',\n",
       " 'children',\n",
       " 'choir',\n",
       " 'christ',\n",
       " 'church',\n",
       " 'city',\n",
       " 'claude',\n",
       " 'colonna',\n",
       " 'constable',\n",
       " 'courbet',\n",
       " 'cranach',\n",
       " 'cross',\n",
       " 'cupid',\n",
       " 'cuyp',\n",
       " 'cycle',\n",
       " 'david',\n",
       " 'death',\n",
       " 'decoration',\n",
       " 'diderot',\n",
       " 'dutch',\n",
       " 'esther',\n",
       " 'fabritius',\n",
       " 'fall',\n",
       " 'family',\n",
       " 'father',\n",
       " 'ferdinand',\n",
       " 'fischer',\n",
       " 'flemish',\n",
       " 'florence',\n",
       " 'flower',\n",
       " 'fountain',\n",
       " 'francesco',\n",
       " 'francia',\n",
       " 'francis',\n",
       " 'french',\n",
       " 'fresco',\n",
       " 'frescoes',\n",
       " 'fruit',\n",
       " 'garden',\n",
       " 'gauguin',\n",
       " 'gentile',\n",
       " 'german',\n",
       " 'giorgione',\n",
       " 'giovanni',\n",
       " 'giuseppe',\n",
       " 'goethe',\n",
       " 'gogh',\n",
       " 'gonzaga',\n",
       " 'hall',\n",
       " 'haman',\n",
       " 'hand',\n",
       " 'hecuba',\n",
       " 'holofernes',\n",
       " 'holy',\n",
       " 'horses',\n",
       " 'italian',\n",
       " 'jacob',\n",
       " 'james',\n",
       " 'jerome',\n",
       " 'john',\n",
       " 'joseph',\n",
       " 'judith',\n",
       " 'jupiter',\n",
       " 'king',\n",
       " 'landscape',\n",
       " 'large',\n",
       " 'leonidas',\n",
       " 'life',\n",
       " 'light',\n",
       " 'lucy',\n",
       " 'madonna',\n",
       " 'magi',\n",
       " 'man',\n",
       " 'mantegna',\n",
       " 'mary',\n",
       " 'mosaic',\n",
       " 'mythological',\n",
       " 'old',\n",
       " 'orazio',\n",
       " 'ossian',\n",
       " 'palazzo',\n",
       " 'paris',\n",
       " 'part',\n",
       " 'paul',\n",
       " 'peter',\n",
       " 'polyphemus',\n",
       " 'portrait',\n",
       " 'portraits',\n",
       " 'predella',\n",
       " 'psyche',\n",
       " 'red',\n",
       " 'religious',\n",
       " 'rolin',\n",
       " 'rome',\n",
       " 'saint',\n",
       " 'saints',\n",
       " 'san',\n",
       " 'saul',\n",
       " 'series',\n",
       " 'sisera',\n",
       " 'small',\n",
       " 'still',\n",
       " 'studio',\n",
       " 'tavern',\n",
       " 'tintoretto',\n",
       " 'town',\n",
       " 'triptych',\n",
       " 'tucher',\n",
       " 'ursula',\n",
       " 'vault',\n",
       " 'venus',\n",
       " 'virgin',\n",
       " 'vision',\n",
       " 'walls',\n",
       " 'water',\n",
       " 'watteau',\n",
       " 'well',\n",
       " 'wheel',\n",
       " 'white',\n",
       " 'wing',\n",
       " 'wolgemut',\n",
       " 'woman',\n",
       " 'world',\n",
       " 'yellow',\n",
       " 'young'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concepts = set()\n",
    "for gs in gt_concepts:\n",
    "    for g in gs.split(\",\"):\n",
    "        concepts.add(g.strip())\n",
    "\n",
    "print(len(concepts))\n",
    "concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e7d815-f489-4771-bca9-7e7bacd0d24d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e1268b-7731-4e54-90e9-f1e0a9355899",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc8e8b8-3037-4f9e-8c14-b15d2d8bc2e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
