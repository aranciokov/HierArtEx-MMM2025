{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447d2177-867d-4cf6-b5f5-6f9d6e1a326f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image  # to visualize images within this notebook\n",
    "import torch  # library used for implementing ML tools, great flexibility and ease of use, highly recommended\n",
    "from PIL import Image as PILImage  # to load images\n",
    "import open_clip  # open source implementation of CLIP, we use it to extract representations of queries and images\n",
    "from torch.utils.data import Dataset, Subset  # useful class for managing datasets, greatly integrated within the pytorch environment\n",
    "import matplotlib.pyplot as plt  # library for plotting\n",
    "from matplotlib.pyplot import figure, subplots, imshow, axis  # library for plotting\n",
    "import os  # contains utilities for reading directories on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f36451-26c3-4ea2-9485-30587c2fd003",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1494d3-eee6-487e-8219-462e910b35b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "indices = pd.read_pickle(\"indices_museum_dataset.pkl\")\n",
    "indices['train'][:10], indices['val'][:10], indices['test'][:10],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abeac0dd-54d9-411e-856b-82a754485fcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class DescriptionSceneMuseum(Dataset):\n",
    "    def __init__(self, data_description_path, data_raw_description_path, data_scene_path, data_art_path, indices, split, customized_margin=False):\n",
    "        self.description_path = data_description_path\n",
    "        self.raw_description_path = data_raw_description_path\n",
    "        self.data_pov_path = data_scene_path\n",
    "        self.indices = indices[split]\n",
    "        self.split = split\n",
    "\n",
    "        available_data = [im.strip(\".pt\") for im in os.listdir(data_scene_path)]\n",
    "        available_data = sorted(available_data)\n",
    "        available_data = [available_data[ix] for ix in self.indices.tolist()]\n",
    "\n",
    "        self.descs = [torch.load(os.path.join(data_description_path, f\"{sm}.pt\")) for sm in available_data]\n",
    "        self.raw_descs = [\" \".join(pd.read_pickle(os.path.join(data_raw_description_path, f\"{sm}.pkl\"))) for sm in available_data]\n",
    "        self.pov_images = [torch.load(os.path.join(data_scene_path, f\"{sm}.pt\")) for sm in available_data]\n",
    "        self.art_vectors = [torch.load(os.path.join(data_art_path, f\"{sm}.pt\")) for sm in available_data]\n",
    "        self.names = available_data\n",
    "        print(f\"'{split.upper()}': {len(self.names)} names, \"\n",
    "              f\"{len(self.descs)} sentences ({sum([len(x) for x in self.descs]) / len(self.descs)} avg), \"\n",
    "              f\"{len(self.pov_images)} images ({sum([len(x) for x in self.pov_images]) / len(self.pov_images)} avg).\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.names)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        desc_tensor = self.descs[index]\n",
    "        if self.split == \"train\":\n",
    "            raw_desc = self.raw_descs[index]\n",
    "        scene_img_tensor = self.pov_images[index]\n",
    "        scene_art_tensor = self.art_vectors[index]\n",
    "        name = self.names[index]\n",
    "\n",
    "        if self.split == \"train\":\n",
    "            return desc_tensor, scene_img_tensor, scene_art_tensor, raw_desc, name, index\n",
    "        else:\n",
    "            return desc_tensor, scene_img_tensor, scene_art_tensor, name, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ef1a2e-2bd5-460e-ad8d-1dacac12c45d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "visual_backbone = \"rn50\"\n",
    "visual_bb_ftsize_k = {'rn18': 512, 'rn34': 512, 'rn50': 2048, 'rn101': 2048, 'vitb16': 768, 'vitb32': 768}\n",
    "visual_bb_ftsize = visual_bb_ftsize_k[visual_backbone]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a1fb8f-2ccc-46a0-a891-4f8ff7ca4fe3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for vn in visual_bb_ftsize_k.keys():\n",
    "    print(vn, torch.load(f\"preextracted_vectors_wikiart_{vn}/Museum1554-7.unity.pt\").shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b19656-3731-49a0-af0a-d28931b2f9b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = DescriptionSceneMuseum(\"./tmp_museums/open_clip_features_museums3k/descriptions/sentences\", \n",
    "                                       \"./tmp_museums/open_clip_features_museums3k/descriptions/tokens_strings\", \n",
    "                                       \"./tmp_museums/open_clip_features_museums3k/images\",\n",
    "                                       f\"./preextracted_vectors_wikiart_{visual_backbone}\",\n",
    "                                indices, \"train\")\n",
    "\n",
    "val_dataset = DescriptionSceneMuseum(\"./tmp_museums/open_clip_features_museums3k/descriptions/sentences\", \n",
    "                                       \"./tmp_museums/open_clip_features_museums3k/descriptions/tokens_strings\", \n",
    "                                       \"./tmp_museums/open_clip_features_museums3k/images\",\n",
    "                                       f\"./preextracted_vectors_wikiart_{visual_backbone}\",\n",
    "                                indices, \"val\")\n",
    "\n",
    "test_dataset = DescriptionSceneMuseum(\"./tmp_museums/open_clip_features_museums3k/descriptions/sentences\", \n",
    "                                       \"./tmp_museums/open_clip_features_museums3k/descriptions/tokens_strings\", \n",
    "                                       \"./tmp_museums/open_clip_features_museums3k/images\",\n",
    "                                       f\"./preextracted_vectors_wikiart_{visual_backbone}\",\n",
    "                                indices, \"test\")\n",
    "desc, scene, art, raw_desc, name, ix = train_dataset[1]\n",
    "print(f\"The sample #{ix} ({name}) has a description of {len(desc)} sentences (shaped as {desc.shape} matrix), whereas there are {len(scene)} images (shaped as {scene.shape} matrix)\")\n",
    "print(f\"Example of raw description (capped at 100 characters): {raw_desc[:100]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2edb539-06cf-4d5c-84f1-c4150e41650c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_descs = [rd for rd in train_dataset.raw_descs] + [rd for rd in val_dataset.raw_descs] + [rd for rd in test_dataset.raw_descs] \n",
    "print(\"tot\", len(all_descs))\n",
    "\n",
    "n_tokens = [len(rd.split()) for rd in all_descs]\n",
    "print(\"avg tokens per museum\", sum(n_tokens) / len(n_tokens))\n",
    "print(\"num tokens\", sum(n_tokens))\n",
    "\n",
    "import re\n",
    "_tmp = [rd.split(\".\") for rd in all_descs]\n",
    "_tmp = [[t for t in ts if \"there are\" in t and \"painting\" in t] for ts in _tmp]\n",
    "_tmp = [[re.sub(r\"In the \\w+ room , there are\", \"\", t).strip() for t in ts] for ts in _tmp]\n",
    "_tmp = [[t for t in ts if len(t.split()) < 3] for ts in _tmp]\n",
    "_tmp = [[t.replace(\" paintings\", \"\") for t in ts] for ts in _tmp]\n",
    "_mm = {'two': 2, 'three': 3, 'four': 4, 'five': 5, 'six': 6}\n",
    "_tmp = [[_mm[t] for t in ts] for ts in _tmp]\n",
    "_x = [sum(t) for t in _tmp]\n",
    "sum(_x) / len(_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874e3b22-c024-48c6-9ec9-b69ad764ace5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cosine_sim(im, s):\n",
    "    '''cosine similarity between all the image and sentence pairs\n",
    "    '''\n",
    "    inner_prod = im.mm(s.t())\n",
    "    im_norm = torch.sqrt((im ** 2).sum(1).view(-1, 1) + 1e-18)\n",
    "    s_norm = torch.sqrt((s ** 2).sum(1).view(1, -1) + 1e-18)\n",
    "    sim = inner_prod / (im_norm * s_norm)\n",
    "    return sim\n",
    "\n",
    "\n",
    "def create_rank(result, entire_descriptor, desired_output_index):\n",
    "    similarity = torch.nn.functional.cosine_similarity(entire_descriptor, result, dim=1)\n",
    "    similarity = similarity.squeeze()\n",
    "    sorted_indices = torch.argsort(similarity, descending=True)\n",
    "    position = torch.where(sorted_indices == desired_output_index)\n",
    "    return position[0].item(), sorted_indices\n",
    "\n",
    "\n",
    "def evaluate(output_description, output_scene, section, out_values=False, excel_format=False):\n",
    "    avg_rank_scene = 0\n",
    "    ranks_scene = []\n",
    "    avg_rank_description = 0\n",
    "    ranks_description = []\n",
    "\n",
    "    ndcg_10_list = []\n",
    "    ndcg_entire_list = []\n",
    "\n",
    "    for j, i in enumerate(output_scene):\n",
    "        rank, sorted_list = create_rank(i, output_description, j)\n",
    "        avg_rank_scene += rank\n",
    "        ranks_scene.append(rank)\n",
    "\n",
    "    for j, i in enumerate(output_description):\n",
    "        rank, sorted_list = create_rank(i, output_scene, j)\n",
    "        avg_rank_description += rank\n",
    "        ranks_description.append(rank)\n",
    "\n",
    "    ranks_scene = np.array(ranks_scene)\n",
    "    ranks_description = np.array(ranks_description)\n",
    "\n",
    "    n_q = len(output_scene)\n",
    "    sd_r1 = 100 * len(np.where(ranks_scene < 1)[0]) / n_q\n",
    "    sd_r5 = 100 * len(np.where(ranks_scene < 5)[0]) / n_q\n",
    "    sd_r10 = 100 * len(np.where(ranks_scene < 10)[0]) / n_q\n",
    "    sd_medr = np.median(ranks_scene) + 1\n",
    "    sd_meanr = ranks_scene.mean() + 1\n",
    "\n",
    "    n_q = len(output_description)\n",
    "    ds_r1 = 100 * len(np.where(ranks_description < 1)[0]) / n_q\n",
    "    ds_r5 = 100 * len(np.where(ranks_description < 5)[0]) / n_q\n",
    "    ds_r10 = 100 * len(np.where(ranks_description < 10)[0]) / n_q\n",
    "    ds_medr = np.median(ranks_description) + 1\n",
    "    ds_meanr = ranks_description.mean() + 1\n",
    "\n",
    "    ds_out, sc_out = \"\", \"\"\n",
    "    for mn, mv in [[\"R@1\", ds_r1],\n",
    "                   [\"R@5\", ds_r5],\n",
    "                   [\"R@10\", ds_r10],\n",
    "                   [\"median rank\", ds_medr],\n",
    "                   [\"mean rank\", ds_meanr],\n",
    "                   ]:\n",
    "        ds_out += f\"{mn}: {mv:.4f}   \"\n",
    "\n",
    "    for mn, mv in [(\"R@1\", sd_r1),\n",
    "                   (\"R@5\", sd_r5),\n",
    "                   (\"R@10\", sd_r10),\n",
    "                   (\"median rank\", sd_medr),\n",
    "                   (\"mean rank\", sd_meanr),\n",
    "                   ]:\n",
    "        sc_out += f\"{mn}: {mv:.4f}   \"\n",
    "\n",
    "    if out_values:\n",
    "        print(section + \" data: \")\n",
    "        print(\"Scenes ranking: \" + ds_out)\n",
    "        print(\"Descriptions ranking: \" + sc_out)\n",
    "    if section == \"test\" and len(ndcg_10_list) > 0:\n",
    "        avg_ndcg_10_entire = 100 * sum(ndcg_10_list) / len(ndcg_10_list)\n",
    "        avg_ndcg_entire = 100 * sum(ndcg_entire_list) / len(ndcg_entire_list)\n",
    "    else:\n",
    "        avg_ndcg_10_entire = -1\n",
    "        avg_ndcg_entire = -1\n",
    "    \n",
    "    if excel_format:\n",
    "        print(\"-\"*5)\n",
    "        print(\"{ds_r1};{ds_r5};{ds_r10};{sd_r1};{sd_r5};{sd_r10};{ds_medr};{sd_medr}\")\n",
    "        print(f\"{ds_r1};{ds_r5};{ds_r10};{sd_r1};{sd_r5};{sd_r10};{ds_medr};{sd_medr}\")\n",
    "        print(\"-\"*5)\n",
    "        formatted_string = f\"{ds_r1};{ds_r5};{ds_r10};{sd_r1};{sd_r5};{sd_r10};{ds_medr};{sd_medr}\"\n",
    "        return ds_r1, ds_r5, ds_r10, sd_r1, sd_r5, sd_r10, avg_ndcg_10_entire, avg_ndcg_entire, ds_medr, sd_medr, formatted_string        \n",
    "    \n",
    "    return ds_r1, ds_r5, ds_r10, sd_r1, sd_r5, sd_r10, avg_ndcg_10_entire, avg_ndcg_entire, ds_medr, sd_medr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8893130a-92d0-42aa-b57b-250bf6b2df5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LossContrastive:\n",
    "    def __init__(self, name, patience=15, delta=.001, verbose=True):\n",
    "        self.train_losses = []\n",
    "        self.validation_losses = []\n",
    "        self.name = name\n",
    "        self.counter_patience = 0\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_score = None\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def on_epoch_end(self, loss, train=True):\n",
    "        if train:\n",
    "            self.train_losses.append(loss)\n",
    "        else:\n",
    "            self.validation_losses.append(loss)\n",
    "\n",
    "    def get_loss_trend(self):\n",
    "        return self.train_losses, self.validation_losses\n",
    "\n",
    "    def calculate_loss(self, pairwise_distances, margin=.25, margin_tensor=None):\n",
    "        batch_size = pairwise_distances.shape[0]\n",
    "        diag = pairwise_distances.diag().view(batch_size, 1)\n",
    "        pos_masks = torch.eye(batch_size).bool().to(pairwise_distances.device)\n",
    "        d1 = diag.expand_as(pairwise_distances)\n",
    "        if margin_tensor is not None:\n",
    "            margin_tensor = margin_tensor.to(pairwise_distances.device)\n",
    "            cost_s = (margin_tensor + pairwise_distances - d1).clamp(min=0)\n",
    "        else:\n",
    "            cost_s = (margin + pairwise_distances - d1).clamp(min=0)\n",
    "        cost_s = cost_s.masked_fill(pos_masks, 0)\n",
    "        cost_s = cost_s / (batch_size * (batch_size - 1))\n",
    "        cost_s = cost_s.sum()\n",
    "\n",
    "        d2 = diag.t().expand_as(pairwise_distances)\n",
    "        if margin_tensor is not None:\n",
    "            margin_tensor = margin_tensor.to(pairwise_distances.device)\n",
    "            cost_d = (margin_tensor + pairwise_distances - d2).clamp(min=0)\n",
    "        else:\n",
    "            cost_d = (margin + pairwise_distances - d2).clamp(min=0)\n",
    "        cost_d = cost_d.masked_fill(pos_masks, 0)\n",
    "        cost_d = cost_d / (batch_size * (batch_size - 1))\n",
    "        cost_d = cost_d.sum()\n",
    "\n",
    "        return (cost_s + cost_d) / 2\n",
    "\n",
    "    def is_val_improving(self):\n",
    "        score = -self.validation_losses[-1] if self.validation_losses else None\n",
    "\n",
    "        if score and self.best_score and self.verbose:\n",
    "            print('epoch:', len(self.validation_losses), ' score:', -score, ' best_score:', -self.best_score, ' counter:', self.counter_patience)\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter_patience += 1\n",
    "            if self.counter_patience >= self.patience:\n",
    "                return False\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter_patience = 0\n",
    "        return True\n",
    "\n",
    "    def save_plots(self):\n",
    "        save_path = f'models/{self.name}.png'\n",
    "        plt.plot(self.train_losses, label='Train Loss')\n",
    "        plt.plot(self.validation_losses, label='Val Loss')\n",
    "\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Loss Trend')\n",
    "\n",
    "        plt.legend()\n",
    "\n",
    "        plt.savefig(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62eee8c1-c538-483a-b450-0eee0d61b445",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "def collate_fn(data):  # data -> desc_tensor, scene_img_tensor, name, index\n",
    "    raw_descs = False\n",
    "    adj = 0\n",
    "    if len(data[0]) == 6:  # train -> raw descriptions\n",
    "        raw_descs = True\n",
    "        adj = 1\n",
    "\n",
    "    tmp_description_povs = [x[0] for x in data]\n",
    "    tmp = pad_sequence(tmp_description_povs, batch_first=True)\n",
    "    descs_pov = pack_padded_sequence(tmp,\n",
    "                                     torch.tensor([len(x) for x in tmp_description_povs]),\n",
    "                                     batch_first=True,\n",
    "                                     enforce_sorted=False)\n",
    "\n",
    "    tmp_pov = [x[1] for x in data]\n",
    "    len_pov = torch.tensor([len(x) for x in tmp_pov])\n",
    "    padded_pov = pad_sequence(tmp_pov, batch_first=True)\n",
    "    padded_pov = torch.transpose(padded_pov, 1, 2)\n",
    "\n",
    "    tmp_art = [x[2] for x in data]\n",
    "    len_art = torch.tensor([len(x) for x in tmp_art])\n",
    "    padded_art = pad_sequence(tmp_art, batch_first=True)\n",
    "    padded_art = torch.transpose(padded_art, 1, 2)\n",
    "\n",
    "    if raw_descs:\n",
    "        raw_descs = [x[3] for x in data]\n",
    "    names = [x[3+adj] for x in data]\n",
    "    indexes = [x[4+adj] for x in data]\n",
    "    \n",
    "    if raw_descs:\n",
    "        return descs_pov, padded_pov, padded_art, raw_descs, names, indexes, len_pov\n",
    "    else:\n",
    "        return descs_pov, padded_pov, padded_art, names, indexes, len_pov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252a26c4-4c2b-43df-a3e4-9d893b114c98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_best_model(model_name, run_folder, *args):\n",
    "    model_path = \"models\"\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    os.makedirs(os.path.join(model_path, run_folder), exist_ok=True)\n",
    "    model_path = os.path.join(model_path, run_folder, model_name + '.pt')\n",
    "    new_dict = dict()\n",
    "    for i, bm in enumerate(args):\n",
    "        new_dict[f'best_model_{str(i)}'] = bm\n",
    "    torch.save(new_dict, model_path)\n",
    "\n",
    "\n",
    "def load_best_model(model_name, run_folder):\n",
    "    model_path = os.path.join(\"models\", run_folder)\n",
    "    avail_models = [m for m in os.listdir(model_path) if m.startswith(model_name)]\n",
    "    assert len(avail_models) == 1, avail_models\n",
    "    model_name_ = avail_models[0]\n",
    "    model_path = model_path + os.sep + model_name_\n",
    "    check_point = torch.load(model_path)\n",
    "    bm_list = [check_point[bm] for bm in check_point.keys()]\n",
    "    return bm_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a64dd4a-80e8-40d7-9347-84185330e954",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36eca79-babe-4a8a-885b-b5b16fa62de5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "    \n",
    "class MyBaseline(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.trf_photo = nn.Linear(in_channels, out_channels)\n",
    "        self.trf_mean = nn.Linear(out_channels, out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, list_length=None, clip_mask=None, imgs_per_room=None):\n",
    "        x = x.to(torch.float32)\n",
    "        \n",
    "        x1 = self.trf_photo(x.transpose(1, 2))\n",
    "        \n",
    "        if clip_mask is not None:\n",
    "            x1 = x1 * clip_mask\n",
    "        # remove the effect of the padding\n",
    "        if list_length is not None:\n",
    "            for item_idx in range(x.shape[0]):\n",
    "                x1[item_idx, list_length[item_idx]:, :] = 0\n",
    "        x1_img = self.relu(x1)\n",
    "        \n",
    "        bsz, max_n_imgs, ft_size = x1_img.shape\n",
    "        list_length_t = torch.tensor(list_length, device=x1_img.device) if isinstance(list_length, list) else list_length.to(x1_img.device)\n",
    "        x1_mean = x1_img.sum(1) / list_length_t.unsqueeze(1)\n",
    "        x1_museum = self.trf_mean(x1_mean)\n",
    "        return x1_museum\n",
    "\n",
    "\n",
    "class GRUNet(nn.Module):\n",
    "    def __init__(self, hidden_size, num_features, is_bidirectional=False):\n",
    "        super(GRUNet, self).__init__()\n",
    "        self.gru = nn.GRU(input_size=num_features, hidden_size=hidden_size, batch_first=True,\n",
    "                          bidirectional=is_bidirectional)\n",
    "        self.is_bidirectional = is_bidirectional\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(torch.float32)\n",
    "        _, h_n = self.gru(x)\n",
    "        if self.is_bidirectional:\n",
    "            return h_n.mean(0)\n",
    "        return h_n.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc226a6b-570b-4fd3-81cd-88844973adee",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### TEST WITH THE BASELINE\n",
    "\n",
    "\n",
    "import wandb\n",
    "use_wandb = True\n",
    "from torch.optim.lr_scheduler import StepLR, CosineAnnealingLR\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import random\n",
    "import string\n",
    "run_folder = ''.join(random.choices(string.ascii_uppercase + string.digits, k=5))\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=True, num_workers=8)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=False, num_workers=4)\n",
    "\n",
    "num_epochs = 50\n",
    "number_of_tries = 3\n",
    "kernel_size = 3\n",
    "final_output_strings = []\n",
    "\n",
    "output_feature_size = 256 # default: 256\n",
    "is_bidirectional = True\n",
    "approach_name = f\"mean_pool_baseline\"\n",
    "\n",
    "for n_try in range(number_of_tries):\n",
    "    lr = 0.001  # default: 0.008\n",
    "    \n",
    "    loss_fn = LossContrastive(approach_name, patience=25, delta=0.0001)\n",
    "\n",
    "    model_desc_pov = GRUNet(hidden_size=output_feature_size, num_features=512, is_bidirectional=is_bidirectional)\n",
    "    model_pov = MyBaseline(in_channels=512, out_channels=256)\n",
    "\n",
    "    model_desc_pov.to(device)\n",
    "    model_pov.to(device)\n",
    "\n",
    "    params = list(model_desc_pov.parameters()) + list(model_pov.parameters())\n",
    "    optimizer = torch.optim.Adam(params, lr=lr)\n",
    "    \n",
    "    step_size = 27\n",
    "    gamma = 0.75\n",
    "    scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "    sched_name = StepLR\n",
    "    \n",
    "    # scheduler = CosineAnnealingLR(optimizer, T_max=50, eta_min=0, last_epoch=-1)\n",
    "    # sched_name = CosineAnnealingLR\n",
    "    \n",
    "    # num_training_steps = (len(train_dataset) * num_epochs) // batch_size\n",
    "    # num_warmup_steps = int(num_training_steps * 0.1)\n",
    "    # scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)\n",
    "    # sched_name = \"cosine_schedule_with_warmup\"\n",
    "    \n",
    "    if use_wandb:\n",
    "        wandb.init(\n",
    "            # set the wandb project where this run will be logged\n",
    "            project=\"Museums\",\n",
    "\n",
    "            # track hyperparameters and run metadata\n",
    "            config={**{\n",
    "                \"batch_size\": batch_size,\n",
    "                \"kernel_size\": kernel_size,\n",
    "                \"learning_rate\": lr,\n",
    "                \"architecture\": MyBaseline,\n",
    "                \"epochs\": num_epochs,\n",
    "                \"approach_name\": approach_name,\n",
    "                \"output_feature_size\": output_feature_size,\n",
    "                \"scheduler/name\": sched_name,\n",
    "            }, **{f\"scheduler/{n}\": v for n, v in scheduler.__dict__.items()}}\n",
    "        )\n",
    "    \n",
    "    \n",
    "    best_r10 = 0\n",
    "    print(f\"({n_try+1}/{number_of_tries}) Train procedure ...\")\n",
    "    for ep in tqdm(range(num_epochs)):\n",
    "        \n",
    "        model_desc_pov.train()\n",
    "        model_pov.train()\n",
    "        # if not loss_fn.is_val_improving():\n",
    "        #     print('Early Stopping !!!')\n",
    "        #     break\n",
    "\n",
    "        total_loss_train = 0\n",
    "        total_loss_val = 0\n",
    "        num_batches_train = 0\n",
    "        num_batches_val = 0\n",
    "\n",
    "        output_description_val = torch.empty(len(indices['val']), output_feature_size)\n",
    "        output_pov_val = torch.empty(len(indices['val']), output_feature_size)\n",
    "\n",
    "        for i, (data_desc_pov, data_pov, data_art, raw_descs, names, indexes, len_pov) in enumerate(train_loader):\n",
    "            data_desc_pov = data_desc_pov.to(device)\n",
    "            data_pov = data_pov.to(device)\n",
    "            data_art = data_art.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            bsz, fts, no_room_times_no_imgs = data_pov.shape\n",
    "\n",
    "            output_desc_pov = model_desc_pov(data_desc_pov)\n",
    "            output_pov = model_pov(data_pov, len_pov)\n",
    "\n",
    "            multiplication_dp = cosine_sim(output_desc_pov, output_pov)\n",
    "\n",
    "            loss_contrastive = loss_fn.calculate_loss(multiplication_dp)\n",
    "\n",
    "            loss_contrastive.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss_train += loss_contrastive.item()\n",
    "            num_batches_train += 1\n",
    "            \n",
    "            # tmp_max_wo_gt = likeness_raw_values.clone()\n",
    "            # tmp_max_wo_gt.fill_diagonal_(0.)\n",
    "            if use_wandb:\n",
    "                wandb.log({\n",
    "                    \"train/loss\": loss_contrastive.item(), \n",
    "                    # \"train/likeness_raw_values_mean\": likeness_raw_values.mean().item(),\n",
    "                    # \"train/likeness_raw_values_min\": likeness_raw_values.min().item(),\n",
    "                    # \"train/likeness_raw_values_max_without_gt\": tmp_max_wo_gt.max().item(),\n",
    "                    \"scheduler/lr\": scheduler.get_last_lr()[0]\n",
    "                })\n",
    "\n",
    "        scheduler.step()\n",
    "        print(scheduler.get_last_lr())\n",
    "        epoch_loss_train = total_loss_train / num_batches_train\n",
    "\n",
    "        model_desc_pov.eval()\n",
    "        model_pov.eval()\n",
    "        # Validation Procedure\n",
    "        with torch.no_grad():\n",
    "            for j, (data_desc_pov, data_pov, data_art, names, indexes, len_pov) in enumerate(val_loader):\n",
    "\n",
    "                data_desc_pov = data_desc_pov.to(device)\n",
    "                data_pov = data_pov.to(device)\n",
    "                data_art = data_art.to(device)\n",
    "\n",
    "                bsz, fts, no_room_times_no_imgs = data_pov.shape\n",
    "\n",
    "                output_desc_pov = model_desc_pov(data_desc_pov)\n",
    "                output_pov = model_pov(data_pov, len_pov)\n",
    "\n",
    "                initial_index = j * batch_size\n",
    "                final_index = (j + 1) * batch_size\n",
    "                if final_index > len(indices['val']):\n",
    "                    final_index = len(indices['val'])\n",
    "\n",
    "                output_description_val[initial_index:final_index, :] = output_desc_pov\n",
    "                output_pov_val[initial_index:final_index, :] = output_pov\n",
    "\n",
    "                multiplication_dp = cosine_sim(output_desc_pov, output_pov)\n",
    "\n",
    "                loss_contrastive = loss_fn.calculate_loss(multiplication_dp)\n",
    "\n",
    "                total_loss_val += loss_contrastive.item()\n",
    "                num_batches_val += 1\n",
    "                if use_wandb:\n",
    "                    wandb.log({\n",
    "                        \"val/batch_loss\": loss_contrastive.item(), \n",
    "                    })\n",
    "\n",
    "            epoch_loss_val = total_loss_val / num_batches_val\n",
    "            if use_wandb:\n",
    "                wandb.log({\n",
    "                    \"val/epoch_loss\": epoch_loss_val, \n",
    "                })\n",
    "\n",
    "    #         print('Loss Train', epoch_loss_train)\n",
    "    #         loss_fn.on_epoch_end(epoch_loss_train, train=True)\n",
    "    #         print('Loss Val', epoch_loss_val)\n",
    "    #         loss_fn.on_epoch_end(epoch_loss_val, train=False)\n",
    "\n",
    "        r1, r5, r10, _, _, _, _, _, _, _ = evaluate(output_description=output_description_val,\n",
    "                                                                  output_scene=output_pov_val, section='val',\n",
    "                                                   out_values=ep % 5 == 4)\n",
    "        \n",
    "        if r10 > best_r10:\n",
    "            best_r10 = r10\n",
    "            save_best_model(f\"{approach_name}_{n_try}\", run_folder, model_pov.state_dict(), model_desc_pov.state_dict())\n",
    "            \n",
    "        if use_wandb:\n",
    "            wandb.log({\n",
    "                \"val/T2S_R@1\": r1, \n",
    "                \"val/T2S_R@5\": r5, \n",
    "                \"val/T2S_R@10\": r10, \n",
    "            })\n",
    "\n",
    "        # Validation ON TRAIN Procedure\n",
    "        output_description_val_train = torch.empty(len(indices['train']), output_feature_size)\n",
    "        output_pov_val_train = torch.empty(len(indices['train']), output_feature_size)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for j, (data_desc_pov, data_pov, data_art, raw_descs, names, indexes, len_pov) in enumerate(train_loader):\n",
    "\n",
    "                data_desc_pov = data_desc_pov.to(device)\n",
    "                data_pov = data_pov.to(device)\n",
    "                data_art = data_art.to(device)\n",
    "\n",
    "                bsz, fts, no_room_times_no_imgs = data_pov.shape\n",
    "\n",
    "                output_desc_pov = model_desc_pov(data_desc_pov)\n",
    "                output_pov = model_pov(data_pov, len_pov)\n",
    "\n",
    "                initial_index = j * batch_size\n",
    "                final_index = (j + 1) * batch_size\n",
    "                if final_index > len(indices['train']):\n",
    "                    final_index = len(indices['train'])\n",
    "\n",
    "                output_description_val_train[initial_index:final_index, :] = output_desc_pov\n",
    "                output_pov_val_train[initial_index:final_index, :] = output_pov\n",
    "\n",
    "                multiplication_dp = cosine_sim(output_desc_pov, output_pov)\n",
    "\n",
    "                loss_contrastive = loss_fn.calculate_loss(multiplication_dp)\n",
    "\n",
    "                total_loss_val += loss_contrastive.item()\n",
    "                num_batches_val += 1\n",
    "\n",
    "            epoch_loss_val = total_loss_val / num_batches_val\n",
    "\n",
    "            print('Loss Train', epoch_loss_train)\n",
    "            loss_fn.on_epoch_end(epoch_loss_train, train=True)\n",
    "            print('Loss Val', epoch_loss_val)\n",
    "            loss_fn.on_epoch_end(epoch_loss_val, train=False)\n",
    "\n",
    "        r1, r5, r10, _, _, _, _, _, _, _ = evaluate(output_description=output_description_val_train,\n",
    "                                                                  output_scene=output_pov_val_train, section='TRAIN',\n",
    "                                                   out_values=ep % 5 == 4)\n",
    "        if use_wandb:\n",
    "            wandb.log({\n",
    "                \"train/T2S_R@1\": r1, \n",
    "                \"train/T2S_R@5\": r5, \n",
    "                \"train/T2S_R@10\": r10, \n",
    "            })\n",
    "        \n",
    "\n",
    "    bm_pov, bm_desc_pov = load_best_model(f\"{approach_name}_{n_try}\", run_folder)\n",
    "    model_pov.load_state_dict(bm_pov)\n",
    "    model_desc_pov.load_state_dict(bm_desc_pov)\n",
    "\n",
    "    test_names = list()\n",
    "    model_pov.eval()\n",
    "    model_desc_pov.eval()\n",
    "    output_description_test = torch.empty(len(indices['test']), output_feature_size)\n",
    "    output_pov_test = torch.empty(len(indices['test']), output_feature_size)\n",
    "    # Evaluate test set\n",
    "    with torch.no_grad():\n",
    "        for j, (data_desc_pov, data_pov, data_art, names, indexes, len_pov) in enumerate(test_loader):\n",
    "\n",
    "            data_desc_pov = data_desc_pov.to(device)\n",
    "            data_pov = data_pov.to(device)\n",
    "            data_art = data_art.to(device)\n",
    "\n",
    "            test_names.extend(names)\n",
    "\n",
    "            bsz, fts, no_room_times_no_imgs = data_pov.shape\n",
    "\n",
    "            output_desc_pov = model_desc_pov(data_desc_pov)\n",
    "            output_pov = model_pov(data_pov, len_pov)\n",
    "\n",
    "            initial_index = j * batch_size\n",
    "            final_index = (j + 1) * batch_size\n",
    "            if final_index > len(indices['test']):\n",
    "                final_index = len(indices['test'])\n",
    "            output_description_test[initial_index:final_index, :] = output_desc_pov\n",
    "            output_pov_test[initial_index:final_index, :] = output_pov\n",
    "    ds1, ds5, ds10, sd1, sd5, sd10, ndgc_10, ndcg, ds_medr, sd_medr, formatted_string = evaluate(\n",
    "        output_description=output_description_test,\n",
    "        output_scene=output_pov_test,\n",
    "        section=\"test\",\n",
    "        out_values=True,\n",
    "        excel_format=True)\n",
    "    \n",
    "    if use_wandb:\n",
    "        wandb.log({\n",
    "            \"test/T2S_R@1\": ds1, \n",
    "            \"test/T2S_R@5\": ds5, \n",
    "            \"test/T2S_R@10\": ds10,\n",
    "            \"test/S2T_R@1\": sd1, \n",
    "            \"test/S2T_R@5\": sd5, \n",
    "            \"test/S2T_R@10\": sd10, \n",
    "        })\n",
    "    \n",
    "    final_output_strings.append(formatted_string)\n",
    "\n",
    "if use_wandb:\n",
    "    wandb.finish()\n",
    "for out_str in final_output_strings:\n",
    "    print(out_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daca2524-fbf7-452e-b670-7bf4b39c4341",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
